{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    \"\"\"Calculates the sign of a number.\"\"\"\n",
    "    if x > 0:\n",
    "        return 1  \n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def create_database(min_value, max_value, d, number_of_points):\n",
    "    \"\"\"Creates a database of random points with a specified range and number of dimensions.\"\"\"\n",
    "    return np.random.uniform(min_value, max_value, (number_of_points, d + 2))\n",
    "\n",
    "def get_target_points(min_value, max_value, d):\n",
    "    \"\"\"Generates random points representing the endpoints of a line.\"\"\"\n",
    "    return np.random.uniform(min_value, max_value, (2, d))\n",
    "\n",
    "def get_target_function(point1, point2):\n",
    "    \"\"\"Calculates the coefficients of the line passing through two random points.\"\"\"\n",
    "    a = (point2[1] - point1[1]) / (point2[0] - point1[0])  # Slope\n",
    "    b = point1[1] - a * point1[0]  # Intercept\n",
    "    return [a, b]\n",
    "\n",
    "def is_above_line(point):\n",
    "    \"\"\"Checks if a point is above a line defined by its coefficients.\"\"\"\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    return np.sign(x**2 + y**2 - 0.6)\n",
    "\n",
    "def target_function_to_database(database, coefficients):\n",
    "    \"\"\"Assigns labels to points in the database based on their position relative to a line.\"\"\"\n",
    "    for i in range(len(database)):\n",
    "        x = database[i][1]\n",
    "        y = database[i][2]\n",
    "\n",
    "        database[i][0] = 1 # Add x0\n",
    "\n",
    "        database[i][-1] = is_above_line([x, y], coefficients) # Add yn\n",
    "    \n",
    "    return database\n",
    "\n",
    "def add_noise(database, noise_ratio=0.1):\n",
    "    \"\"\"Adds noise to the labels of a given ratio of the database.\"\"\"\n",
    "    num_noisy_points = int(noise_ratio * len(database))\n",
    "    noisy_indices = np.random.choice(len(database), num_noisy_points, replace=False)\n",
    "    database[noisy_indices, -1] = -1 * database[noisy_indices, -1]\n",
    "    return database\n",
    "\n",
    "def calculate_out_of_sample_error(g, test_database):\n",
    "    \"\"\"Calculates the out-of-sample error of a given hypothesis on a test database.\"\"\"\n",
    "    X_test = test_database[:, :3]  \n",
    "    y_test = test_database[:, -1]  \n",
    "\n",
    "    predictions = np.sign(X_test @ g)\n",
    "    error_outside = np.mean(predictions != y_test)\n",
    "    return error_outside\n",
    "\n",
    "def linear_regression(d, target_train_database):\n",
    "    \"\"\"Calculates the linear regression of a given database.\"\"\"\n",
    "    \n",
    "    X = target_train_database[:, :d+1]  \n",
    "    y = target_train_database[:, -1]    \n",
    "\n",
    "    # Calculates (X^T X)^-1 X^T y\n",
    "    X_transpose = X.T\n",
    "    pseudo_inverse = np.linalg.inv(X_transpose @ X) @ X_transpose\n",
    "\n",
    "    g = pseudo_inverse @ y\n",
    "    predictions = np.sign(X @ g)\n",
    "    error_inside = np.mean(predictions != y)\n",
    "    \n",
    "    return g, error_inside\n",
    "\n",
    "def run_non_linear_regression(d, N, number_of_executions, transformation = False, hypotheses=None):\n",
    "    \"\"\"Executes the non-linear regression algorithm, accepting a transformation of the features.\"\"\"\n",
    "    sum_error_inside = 0\n",
    "    sum_ws = np.zeros(6)\n",
    "    for _ in range(number_of_executions):\n",
    "        # Criação do banco de dados\n",
    "        train_database = create_database(-1, 1, d, N)\n",
    "        x1 = train_database[:, 1]\n",
    "        x2 = train_database[:, 2]\n",
    "\n",
    "        # Aplicação da função target\n",
    "        train_database[:, 3] = is_above_line([x1, x2])\n",
    "\n",
    "        # Adição de ruído\n",
    "        train_database = add_noise(train_database)\n",
    "\n",
    "        if transformation:\n",
    "            train_database = transform_features(x1, x2)\n",
    "            w, _ = linear_regression(5, train_database)\n",
    "            sum_ws += w\n",
    "\n",
    "            predicted_values = np.sign(hypotheses[0] + hypotheses[1] * x1 + hypotheses[2] * x2 + hypotheses[3] * x1 * x2 + hypotheses[4] * x1**2 + hypotheses[5] * x2**2)\n",
    "            real_values = train_database[:, -1]\n",
    "\n",
    "            error_inside = np.mean(predicted_values != real_values)\n",
    "\n",
    "        else:\n",
    "            w, error_inside = linear_regression(2, train_database)\n",
    "       \n",
    "        sum_error_inside += error_inside\n",
    "\n",
    "\n",
    "    if transformation:\n",
    "        distance = compare_hypotheses(sum_ws / number_of_executions, hypotheses)\n",
    "        return distance, sum_error_inside / number_of_executions\n",
    "\n",
    "    return sum_error_inside / number_of_executions\n",
    "\n",
    "def transform_features(x1, x2):\n",
    "    \"\"\"Transform the features of the database.\"\"\"\n",
    "    return np.column_stack((np.ones_like(x1), x1, x2, x1 * x2, x1**2, x2**2, is_above_line([x1, x2])))\n",
    "\n",
    "def compare_hypotheses(w, hypothese):\n",
    "    \"\"\"Calculates the distance between two hypotheses.\"\"\"\n",
    "    distance = np.linalg.norm(w - hypothese)\n",
    "    return distance\n",
    "\n",
    "def estimate_e_out(hypothesis, num_points=1000, num_executions=1000):\n",
    "    \"\"\"Estimates the out-of-sample error of a given hypothesis.\"\"\"\n",
    "    sum_error_outside = 0\n",
    "\n",
    "    for _ in range(num_executions):\n",
    "        # Gerar um novo conjunto de 1000 pontos\n",
    "        x1 = np.random.uniform(-1, 1, size=num_points)\n",
    "        x2 = np.random.uniform(-1, 1, size=num_points)\n",
    "        \n",
    "        # Calcular os valores previstos usando a hipótese fornecida\n",
    "        predicted_values = np.sign(-1 - 0.05 * x1 + 0.08 * x2 + 0.13 * x1 * x2 + 1.5 * x1**2 + 1.5 * x2**2)\n",
    "        \n",
    "        # Gerar os rótulos reais com base na função alvo\n",
    "        real_values = np.sign(x1**2 + x2**2 - 0.6)\n",
    "        \n",
    "        # Add noise\n",
    "        noise_database = add_noise(np.column_stack((x1, x2, real_values)))\n",
    "        real_values = noise_database[:, -1]\n",
    "        \n",
    "        # Calcular o erro de classificação fora-da-amostra\n",
    "        error_outside = np.mean(predicted_values != real_values)\n",
    "        \n",
    "        sum_error_outside += error_outside\n",
    "    \n",
    "    return sum_error_outside / num_executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Execute a Regressão Linear sem nenhuma transformação, usando o vetor de atributos(1, x2, x2) para encontrar o peso w. Qual é o valor aproximado de classificação do erro médio dentro-de-amostra Ein (medido ao longo de 1000 execuções)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-linear regression for N = 1000\n",
      "Mean in-sample error:  0.47979100000000063\n"
     ]
    }
   ],
   "source": [
    "mean_in_error = run_non_linear_regression(2, 1000, 1000, transformation=False)\n",
    "print(\"Non-linear regression for N = 1000\")\n",
    "print(\"Mean in-sample error: \", mean_in_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Agora, transforme os N = 1000 dados de treinamento seguindo o vetor de atributos não-linear (1, x2, x2, x1x2, x1<sup>2</sup>, x2<sup>2</sup>). Encontre o vetor we que corresponda à solução da Regressão Linear. Quais das hipóteses a seguir é a mais próxima à que você encontrou? Avalie o resultado médio obtido após 1000 execuções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) g(x1, x2) = sign(-1 - 0.05x1 + 0.08x2 + 0.13x1x2 + 1.5x1^2 + 1.5x2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between mean weights and target function (a):  0.6966509863924774\n",
      "Mean in-sample error:  0.05354699999999978\n"
     ]
    }
   ],
   "source": [
    "distance, error_inside = run_non_linear_regression(2, 1000, 1000, transformation=True, hypotheses=np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5]))\n",
    "print(\"Distance between mean weights and target function (a): \", distance)\n",
    "print(\"Mean in-sample error: \", error_inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) g(x1, x2) = sign(-1 - 0.05x1 + 0.08x2 + 0.13x1x2 + 1.5x1^2 + 15x2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between mean weights and target function (b):  13.059743542757625\n",
      "Mean in-sample error:  0.31115599999999977\n"
     ]
    }
   ],
   "source": [
    "distance, error_inside  = run_non_linear_regression(2, 1000, 1000, transformation=True, hypotheses=np.array([-1, -0.05, 0.08, 0.13, 1.5, 15]))\n",
    "print(\"Distance between mean weights and target function (b): \", distance)\n",
    "print(\"Mean in-sample error: \", error_inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) g(x1, x2) = sign(-1 - 0.05x1 + 0.08x2 + 0.13x1x2 + 15x1^2 + 1.5x2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between mean weights and target function (c):  13.064652795918663\n",
      "Mean in-sample error:  0.31104799999999966\n"
     ]
    }
   ],
   "source": [
    "distance, error_inside = run_non_linear_regression(2, 1000, 1000, transformation=True, hypotheses=np.array([-1, -0.05, 0.08, 0.13, 15, 1.5]))\n",
    "print(\"Distance between mean weights and target function (c): \", distance)\n",
    "print(\"Mean in-sample error: \", error_inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) g(x1, x2) = sign(-1 - 1.5x1 + 0.08x2 + 0.13x1x2 + 0.05x1^2 + 0.05x2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between mean weights and target function (d):  3.0886899516425976\n",
      "Mean in-sample error:  0.38540699999999994\n"
     ]
    }
   ],
   "source": [
    "distance, error_inside = run_non_linear_regression(2, 1000, 1000, transformation=True, hypotheses=np.array([-1, -1.5, 0.08, 0.13, 0.05, 0.05]))\n",
    "print(\"Distance between mean weights and target function (d): \", distance)\n",
    "print(\"Mean in-sample error: \", error_inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) g(x1, x2) = sign(-1 - 0.05x1 + 0.08x2 + 1.5x1x2 + 0.15x1^2 + 0.15x2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between mean weights and target function (e):  2.9629257131505056\n",
      "Mean in-sample error:  0.4679380000000008\n"
     ]
    }
   ],
   "source": [
    "distance, error_inside = run_non_linear_regression(2, 1000, 1000, transformation=True, hypotheses=np.array([-1, -0.05, 0.08, 1.5, 0.15, 0.15]))\n",
    "print(\"Distance between mean weights and target function (e): \", distance)\n",
    "print(\"Mean in-sample error: \", error_inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Qual o valor mais próximo do erro de classificação fora-de-amostra Eout de sua hipótese na questão anterior? (Estime-o gerando um novo conjunto de 1000 pontos e usando 1000 execuções diferentes, como antes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated out-of-sample error for hyphotesis (a):  0.14252900000000038\n"
     ]
    }
   ],
   "source": [
    "error_outside = estimate_e_out(np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5]))\n",
    "print(\"Estimated out-of-sample error for hyphotesis (a): \", error_outside)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACELab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
